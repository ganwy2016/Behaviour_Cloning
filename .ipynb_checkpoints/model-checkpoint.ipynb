{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Steps to build the network:\n",
    "\n",
    "1. Load the training data and do a train/validation split.\n",
    "2. Preprocess data.\n",
    "3. Build a convolutional neural network to classify traffic signs.\n",
    "4. Build a feedforward neural network to classify traffic signs.\n",
    "5. Evaluate performance of final neural network on testing data.\n",
    "\n",
    "Keep an eye on the network’s accuracy over time. Once the accuracy reaches the 98% range, you can be confident that you’ve built and trained an effective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center       object\n",
      "left         object\n",
      "right        object\n",
      "steering    float32\n",
      "throttle    float32\n",
      "brake       float32\n",
      "speed       float32\n",
      "dtype: object\n",
      "Training data size =  8036\n",
      "Training labels size =  8036\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the data by reading the logfile\n",
    "\n",
    "data = pd.read_csv('data/data/driving_log.csv',dtype={'center': str, 'left':str,'right':str,'steering': np.float32,'throttle': np.float32,'brake': np.float32,'speed': np.float32})\n",
    "                   #converters={'category',\"Price\":int} dtype='category')\n",
    "print(data.dtypes)\n",
    "\n",
    "X_train = data['center']\n",
    "y_train = data['steering']\n",
    "\n",
    "print('Training data size = ', len(X_train))\n",
    "print('Training labels size = ',len(y_train))\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to pickle file: data.p\n",
      "Data saved to pickle file: labels.p\n"
     ]
    }
   ],
   "source": [
    "# preprocess all data and save to pickle files\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "    \n",
    "def normalise(im):\n",
    "    width,height = im.size\n",
    "    im = np.array(im.resize((round(width*0.5),round(height*0.5)), Image.ANTIALIAS)) # reduce to half size\n",
    "    im = im/255 - 0.5 # normalise data\n",
    "    return im\n",
    "        \n",
    "def append_pickle (pickle_filename, data):\n",
    "    with open(pickle_filename,'ab') as wfp:\n",
    "        pickle.dump(data, wfp)\n",
    "\n",
    "def process_images (data, pickle_filename):\n",
    "    for filename in data:\n",
    "        #print('data/data/'+filename)\n",
    "        im = Image.open('data/data/'+filename)\n",
    "        width,height = im.size\n",
    "        #print(width,',',height)\n",
    "        #plt.imshow(im);\n",
    "        #plt.show()\n",
    "        im = normalise(im)\n",
    "        append_pickle(pickle_filename, im)\n",
    "    print('Data saved to pickle file:',pickle_filename)\n",
    "    \n",
    "def process_labels (data, pickle_filename):\n",
    "    append_pickle(pickle_filename, data)\n",
    "    print('Data saved to pickle file:',pickle_filename)\n",
    "\n",
    "process_images (X_train[1:6],'data.p') # note: here images are saved one by one\n",
    "process_labels (y_train[1:6],'labels.p') # note: labels are saved all in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# load pickled data which was written in batches\n",
    "X_train_t = []\n",
    "y_train_t = []\n",
    "\n",
    "def read_pickle_file(filename):\n",
    "    data_read = []\n",
    "    if os.path.exists(filename):\n",
    "        # \"with\" statements are very handy for opening files. \n",
    "        with open(filename,'rb') as rfp:\n",
    "            while 1:\n",
    "                if len(data_read)<1:\n",
    "                    data_read = pickle.load(rfp)\n",
    "                else:\n",
    "                    try:\n",
    "                        data_read = data_read + pickle.load(rfp)\n",
    "                    except EOFError:\n",
    "                        break # no more data in the file\n",
    "        print('Data loaded')    \n",
    "    else:\n",
    "        print('ERROR - File not found!')\n",
    "    return data_read\n",
    "\n",
    "X_train_t = read_pickle_file('data.p')\n",
    "y_train_t = read_pickle_file('labels.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38400\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(np.size(X_train_t))\n",
    "print(len(y_train_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitted into train and validation data\n",
      "X_train contains just the path at the moment\n",
      "X_val contains just the path at the moment\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train, validate and test data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print('Data splitted into train and validation data')\n",
    "print('X_train contains just the path at the moment')\n",
    "print('X_val contains just the path at the moment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size =  6428\n",
      "Training labels size =  6428\n",
      "Validation data size =  1608\n",
      "Validation labels size =  1608\n"
     ]
    }
   ],
   "source": [
    "print('Training data size = ', len(X_train))\n",
    "print('Training labels size = ',len(y_train))\n",
    "print('Validation data size = ',len(X_val))\n",
    "print('Validation labels size = ',len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[365, 703, 967, 12, 78, 1029, 1552, 421, 474, 1172]\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "batch_size = 10\n",
    "x = [i for i in range(len(X_val))]\n",
    "shuffle(x)\n",
    "indices = x[0:batch_size]\n",
    "    \n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_im' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-f0193adb0a10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_im' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "train_data = {'data': X_train_im, 'label': y_train }\n",
    "batch_size = 100\n",
    "count = 0\n",
    "while count < (len(train_data)-batch_size):\n",
    "    with open('train.p','ab') as wfp:\n",
    "        pickle.dump(train_data[count:count+batch_size], wfp)\n",
    "    count = count+batch_size\n",
    "    print('.')\n",
    "with open('texto','ab') as wfp:\n",
    "    pickle.dump(train_data[count:len(train_data)], wfp)\n",
    "    \n",
    "print('train.p file saved')\n",
    "\n",
    "val_data = { 'data': X_val_im, 'label': y_val }\n",
    "count = 0\n",
    "while count < (len(val_data)-batch_size):\n",
    "    with open('validation.p','ab') as wfp:\n",
    "        pickle.dump(val_data[count:count+batch_size], wfp)\n",
    "    count = count+batch_size\n",
    "    print('.')\n",
    "with open('texto','ab') as wfp:\n",
    "    pickle.dump(val_data[count:len(val_data)], wfp)\n",
    "\n",
    "print('validation.p file saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Nino', '10')\n",
      "('Tom', '20')\n",
      "('TT', '20')\n",
      "('OO', '11')\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(high_scores_filename):\n",
    "    # \"with\" statements are very handy for opening files. \n",
    "    with open(high_scores_filename,'rb') as rfp:\n",
    "        while 1:\n",
    "            try:\n",
    "                read_data = pickle.load(rfp)\n",
    "                print(read_data)\n",
    "            except EOFError:\n",
    "                break # no more data in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-aa687fd87877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"train.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.p file saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_val_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "train_data = { \"data\": X_train_im, \"label\": y_train }\n",
    "pickle.dump( train_data, open( \"train.p\", \"ab\" ) )\n",
    "print('train.p file saved')\n",
    "val_data = { \"data\": X_val_im, \"label\": y_val }\n",
    "pickle.dump( val_data, open( \"validation.p\", \"wb\" ) )\n",
    "print('validation.p file saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6428\n",
      "6428\n",
      "1608\n",
      "1608\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "loaded_data = pickle.load( open( \"train.p\", \"rb\" ) )\n",
    "train_X = loaded_data['data']\n",
    "train_y = loaded_data['label']\n",
    "print(len(train_X))\n",
    "print(len(train_y))\n",
    "loaded_data = pickle.load( open( \"validation.p\", \"rb\" ) )\n",
    "val_X = loaded_data['data']\n",
    "val_y = loaded_data['label']\n",
    "print(len(val_X))\n",
    "print(len(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the images from IMG folder and save them\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "from numpy import array\n",
    "\n",
    "image_list = []\n",
    "filename_list = []\n",
    "for filename in glob.glob('new_treated_images/*.png'):\n",
    "    filename_list.append(filename)\n",
    "    im = cv2.imread(filename)\n",
    "    im = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n",
    "    im = cv2.resize(im,(32,32),interpolation = cv2.INTER_AREA)\n",
    "    image_list.append(im)\n",
    "\n",
    "print(\"There are \",len(image_list),\"images in the folder.\")\n",
    "plotImageSet(image_list)\n",
    "\n",
    "for name in filename_list:\n",
    "    print(name)\n",
    "\n",
    "# preprocess the images\n",
    "image_list_gray = []\n",
    "for image in image_list:\n",
    "    image_list_gray.append((rgb2gray(image)-128)/128,);\n",
    "\n",
    "plotImageSet(image_list_gray,'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a multi-input model with 10 classes:\n",
    "\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "merged = Merge([left_branch, right_branch], mode='concat')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(merged)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# since the model has 2 inputs\n",
    "model.fit([data_1, data_2], labels, nb_epoch=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert model to json format\n",
    "json_string = model.to_json()\n",
    "model = model_from_json(json_string)\n",
    "\n",
    "#save model to files\n",
    "model.save_weights(model.h5)\n",
    "with open('model.json', 'w') as outfile:\n",
    "    json.dump(model, outfile)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
