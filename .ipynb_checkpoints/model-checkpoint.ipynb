{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Steps to build the network:\n",
    "\n",
    "1. Load the training data and do a train/validation split.\n",
    "2. Preprocess data.\n",
    "3. Build a convolutional neural network to classify traffic signs.\n",
    "4. Build a feedforward neural network to classify traffic signs.\n",
    "5. Evaluate performance of final neural network on testing data.\n",
    "\n",
    "Keep an eye on the network’s accuracy over time. Once the accuracy reaches the 98% range, you can be confident that you’ve built and trained an effective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center       object\n",
      "left         object\n",
      "right        object\n",
      "steering    float32\n",
      "throttle    float32\n",
      "brake       float32\n",
      "speed       float32\n",
      "dtype: object\n",
      "Training data size =  8036\n",
      "Training labels size =  8036\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the data by reading the logfile\n",
    "\n",
    "data = pd.read_csv('data/data/driving_log.csv',dtype={'center': str, 'left':str,'right':str,'steering': np.float32,'throttle': np.float32,'brake': np.float32,'speed': np.float32})\n",
    "                   #converters={'category',\"Price\":int} dtype='category')\n",
    "print(data.dtypes)\n",
    "\n",
    "X_train = data['center']\n",
    "y_train = data['steering']\n",
    "\n",
    "print('Training data size = ', len(X_train))\n",
    "print('Training labels size = ',len(y_train))\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images processed and stored in array\n"
     ]
    }
   ],
   "source": [
    "# preprocess all data and save to pickle files\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "    \n",
    "def normalise(im):\n",
    "    width,height = im.size\n",
    "    im = np.array(im.resize((round(width*0.5),round(height*0.5)), Image.ANTIALIAS)) # reduce to half size\n",
    "    im = im/255 - 0.5 # normalise data\n",
    "    return im\n",
    "        \n",
    "def append_pickle (pickle_filename, data):\n",
    "    with open(pickle_filename,'ab') as wfp:\n",
    "        pickle.dump(data, wfp)\n",
    "\n",
    "def process_images (data, pickle_filename):\n",
    "    X_train_im = []\n",
    "    for filename in data:\n",
    "        #print('data/data/'+filename)\n",
    "        im = Image.open('data/data/'+filename)\n",
    "        #plt.imshow(im);\n",
    "        #plt.show()\n",
    "        im = normalise(im)\n",
    "        X_train_im.append(im)\n",
    "    return X_train_im\n",
    "        #append_pickle(pickle_filename, im)\n",
    "    #print('Data saved to pickle file:',pickle_filename)\n",
    "    \n",
    "def process_labels (data, pickle_filename):\n",
    "    append_pickle(pickle_filename, data)\n",
    "    print('Data saved to pickle file:',pickle_filename)\n",
    "\n",
    "X_train_im = process_images (X_train,'data.p') # note: here images are saved one by one\n",
    "#process_labels (y_train[1:6],'labels.p') # note: labels are saved all in one go\n",
    "print ('Images processed and stored in array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data saved to pickle file\n",
      "labels saved to pickle file\n"
     ]
    }
   ],
   "source": [
    "#save data to pickle files\n",
    "pickle_filename = 'data.p'\n",
    "with open(pickle_filename,'wb') as wfp:\n",
    "    pickle.dump(X_train_im, wfp)\n",
    "print('data saved to pickle file')\n",
    "pickle_filename = 'labels.p'\n",
    "with open(pickle_filename,'wb') as wfp:\n",
    "    pickle.dump(y_train, wfp)\n",
    "print('labels saved to pickle file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8036\n",
      "8036\n"
     ]
    }
   ],
   "source": [
    "# read data from pickle files\n",
    "X_train = []\n",
    "with open('data.p','rb') as rfp:\n",
    "    X_train = pickle.load(rfp)\n",
    "y_train = []\n",
    "with open('labels.p','rb') as rfp:\n",
    "    y_train = pickle.load(rfp)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitted into train and validation data\n",
      "X_train contains just the path at the moment\n",
      "X_val contains just the path at the moment\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train, validate and test data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print('Data splitted into train and validation data')\n",
    "print('X_train contains just the path at the moment')\n",
    "print('X_val contains just the path at the moment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size =  6428\n",
      "Training labels size =  6428\n",
      "Validation data size =  1608\n",
      "Validation labels size =  1608\n"
     ]
    }
   ],
   "source": [
    "print('Training data size = ', len(X_train))\n",
    "print('Training labels size = ',len(y_train))\n",
    "print('Validation data size = ',len(X_val))\n",
    "print('Validation labels size = ',len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[365, 703, 967, 12, 78, 1029, 1552, 421, 474, 1172]\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "batch_size = 10\n",
    "x = [i for i in range(len(X_val))]\n",
    "shuffle(x)\n",
    "indices = x[0:batch_size]\n",
    "    \n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 160, 3)\n",
      "38400\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (80, 160, 3)\n",
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
    "# define the model\n",
    "print('Input shape:',(X_train[0].shape))\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 3, 3, input_shape=X_train[0].shape))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add((Dropout(0.5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(43, activation='softmax'))\n",
    "# for a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert model to json format\n",
    "json_string = model.to_json()\n",
    "model = model_from_json(json_string)\n",
    "\n",
    "#save model to files\n",
    "model.save_weights(model.h5)\n",
    "with open('model.json', 'w') as outfile:\n",
    "    json.dump(model, outfile)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
